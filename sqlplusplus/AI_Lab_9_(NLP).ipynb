{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Stop-word Filteration**"
      ],
      "metadata": {
        "id": "wS78Caf8ANbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=input()\n",
        "documents=[a]\n",
        "while(a!='exit'):\n",
        "    a=input()\n",
        "    documents.append(a)\n",
        "\n",
        "lower_case_documents = []\n",
        "for i in documents:\n",
        "    lower_case_documents.append(i.lower())\n",
        "print(lower_case_documents)"
      ],
      "metadata": {
        "id": "e-TJ7voSuUMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16cfb0d4-3053-4336-d6cf-8161f9d4e467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "This is a sample sentence, showing off the stopwords filteration\n",
            "exit\n",
            "['', '', '', '', 'this is a sample sentence, showing off the stopwords filteration', 'exit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sans_punctuation_documents = []\n",
        "import string\n",
        "\n",
        "for i in lower_case_documents:\n",
        "    sans_punctuation_documents.append(''.join(c for c in i if c not in string.punctuation))\n",
        "    \n",
        "print(sans_punctuation_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oLBjmRNzDKm",
        "outputId": "550c9a0e-6fe9-49a1-d8ff-616aa710670f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '', '', '', 'this is a sample sentence showing off the stopwords filteration', 'exit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_documents = []\n",
        "for i in sans_punctuation_documents:\n",
        "    preprocessed_documents.append(i.split(' '))\n",
        "print(preprocessed_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbT2uaiezm4o",
        "outputId": "352f7f0c-8694-4f21-e929-91277d0bfb45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[''], [''], [''], [''], ['this', 'is', 'a', 'sample', 'sentence', 'showing', 'off', 'the', 'stopwords', 'filteration'], ['exit']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frequency_list = []\n",
        "import pprint\n",
        "from collections import Counter\n",
        "\n",
        "for i in preprocessed_documents:\n",
        "    frequency_list.append(Counter(i))\n",
        "    \n",
        "pprint.pprint(frequency_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q0-ft8Gzv5P",
        "outputId": "94af573a-f7ed-4358-edca-5a4cc6392139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Counter({'': 1}),\n",
            " Counter({'': 1}),\n",
            " Counter({'': 1}),\n",
            " Counter({'': 1}),\n",
            " Counter({'this': 1,\n",
            "          'is': 1,\n",
            "          'a': 1,\n",
            "          'sample': 1,\n",
            "          'sentence': 1,\n",
            "          'showing': 1,\n",
            "          'off': 1,\n",
            "          'the': 1,\n",
            "          'stopwords': 1,\n",
            "          'filteration': 1}),\n",
            " Counter({'exit': 1})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "QDwBPQyVzyfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "\n",
        "print(vectorizer.fit(documents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzeaFLUNz7Q3",
        "outputId": "e208fe8b-d8a6-49b7-a940-cbc1af5fa90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary: \", vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB1fggNDz9uU",
        "outputId": "17b6c935-8df5-4bbd-dd7d-fb6f42c11716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:  {'this': 9, 'is': 2, 'sample': 4, 'sentence': 5, 'showing': 6, 'off': 3, 'the': 8, 'stopwords': 7, 'filteration': 1, 'exit': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = vectorizer.transform(documents)"
      ],
      "metadata": {
        "id": "FXATr2E2z_dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Encoded Document is:\")\n",
        "print(vector.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOvK5fQd0C5n",
        "outputId": "a70ae6d6-4ab4-4643-a11f-324495f78488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Document is:\n",
            "[[0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 1 1 1 1 1 1 1 1]\n",
            " [1 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis**"
      ],
      "metadata": {
        "id": "w9-FhIt7ABgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FkxlMZL_-Qy",
        "outputId": "1ddeb086-6c4b-4bee-a664-6fe5d3b76d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "wg-xfCJiAHgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "text = \"Former US President Donald Trump has shared a video stating that 'camping' will be banned wherever possible. He said that those found sleeping in unsanctioned areas will be arrested, adding that people will be given a choice to either enter treatment programmes or go to jail. Trump proposed setting up large tent cities to combat homelessness.\"\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Remove stop words and lemmatize remaining words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n",
        "\n",
        "# Calculate the frequency distribution of the remaining words\n",
        "fdist = FreqDist(tokens)\n",
        "\n",
        "# Calculate sentiment score using NLTK's SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment = sia.polarity_scores(text)\n",
        "print(f\"The sentiment of the text is: {sentiment['compound']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvgcQEj-_-w2",
        "outputId": "5307747c-5b39-4244-b55a-9402901d2451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the text is: -0.7269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T-6-mF6zAJ2N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}